<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-XB18VJ73B7"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-XB18VJ73B7');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Suvash Sedhain | DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference</title>

    <!-- Open Graph -->
    <meta property="og:title" content="DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference" />
    <meta property="og:description" content="How DeepSeek uses idle decode-side NICs to double KV-Cache loading throughput in prefill-decode disaggregated serving." />
    <meta property="og:url" content="https://mesuvash.github.io/blog/2026/dualpath/" />
    <meta property="og:type" content="article" />

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference" />
    <meta name="twitter:description" content="How DeepSeek uses idle decode-side NICs to double KV-Cache loading throughput in prefill-decode disaggregated serving." />

    <link rel="shortcut icon" href="/assets/img/favicon.ico">
    <link rel="stylesheet" href="/assets/css/blog.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}]});"></script>
</head>
<body>

<header class="site-header">
  <div class="header-inner">
    <a href="/" class="site-title">Suvash Sedhain</a>
    <nav class="site-nav">
      <a href="/">About</a>
      <a href="/blog/" class="active">Blog</a>
      <a href="/projects/">Projects</a>
      <a href="/publications/">Publications</a>
    </nav>
  </div>
</header>

<div class="page-content">

<header class="post-header">
    <h1>DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference</h1>
    <p class="post-meta">February 26, 2026</p>
    <p class="follow-links">Follow me on <a href="https://x.com/suvsh" target="_blank" rel="noopener">X</a> and <a href="https://www.linkedin.com/in/suvashsedhain/" target="_blank" rel="noopener">LinkedIn</a></p>
    <div class="share-buttons">
        <span>Share:</span>
        <a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fmesuvash.github.io%2Fblog%2F2026%2Fdualpath%2F&text=DualPath%3A%20Breaking%20the%20Storage%20Bandwidth%20Bottleneck%20in%20Agentic%20LLM%20Inference" target="_blank" rel="noopener" title="Share on X">
            <svg viewBox="0 0 24 24"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
        </a>
        <a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fmesuvash.github.io%2Fblog%2F2026%2Fdualpath%2F" target="_blank" rel="noopener" title="Share on LinkedIn">
            <svg viewBox="0 0 24 24"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
        </a>
        <a href="https://www.reddit.com/submit?url=https%3A%2F%2Fmesuvash.github.io%2Fblog%2F2026%2Fdualpath%2F&title=DualPath%3A%20Breaking%20the%20Storage%20Bandwidth%20Bottleneck%20in%20Agentic%20LLM%20Inference" target="_blank" rel="noopener" title="Share on Reddit">
            <svg viewBox="0 0 24 24"><path d="M12 0A12 12 0 000 12a12 12 0 0012 12 12 12 0 0012-12A12 12 0 0012 0zm5.01 4.744c.688 0 1.25.561 1.25 1.249a1.25 1.25 0 01-2.498.056l-2.597-.547-.8 3.747c1.824.07 3.48.632 4.674 1.488.308-.309.73-.491 1.207-.491.968 0 1.754.786 1.754 1.754 0 .716-.435 1.333-1.01 1.614a3.111 3.111 0 01.042.52c0 2.694-3.13 4.87-7.004 4.87-3.874 0-7.004-2.176-7.004-4.87 0-.183.015-.366.043-.534A1.748 1.748 0 014.028 12c0-.968.786-1.754 1.754-1.754.463 0 .898.196 1.207.49 1.207-.883 2.878-1.43 4.744-1.487l.885-4.182a.342.342 0 01.14-.197.35.35 0 01.238-.042l2.906.617a1.214 1.214 0 011.108-.701zM9.25 12C8.561 12 8 12.562 8 13.25c0 .687.561 1.248 1.25 1.248.687 0 1.248-.561 1.248-1.249 0-.688-.561-1.249-1.249-1.249zm5.5 0c-.687 0-1.248.561-1.248 1.25 0 .687.561 1.248 1.249 1.248.688 0 1.249-.561 1.249-1.249 0-.687-.562-1.249-1.25-1.249zm-5.466 3.99a.327.327 0 00-.231.094.33.33 0 000 .463c.842.842 2.484.913 2.961.913.477 0 2.105-.056 2.961-.913a.361.361 0 000-.463.327.327 0 00-.231-.094c-.06 0-.12.02-.169.07-.572.573-1.7.76-2.546.76-.846 0-1.989-.187-2.561-.76a.291.291 0 00-.184-.07z"/></svg>
        </a>
    </div>
</header>

<article class="post-content">

<p class="subtitle">How DeepSeek uses idle decode-side NICs to double KV-Cache loading throughput in prefill-decode disaggregated serving.</p>

<div class="toc">
    <h3>Contents</h3>
    <ol>
        <li><a href="#problem">The Problem: Storage NICs Can't Keep Up</a></li>
        <li><a href="#background">Background: PD Disaggregation and Agentic Workloads</a></li>
        <li><a href="#insight">The Key Insight</a></li>
        <li><a href="#architecture">DualPath Architecture</a></li>
        <li><a href="#traffic">CNIC-Centric Traffic Manager</a></li>
        <li><a href="#scheduler">Adaptive Request Scheduler</a></li>
        <li><a href="#results">Results</a></li>
        <li><a href="#practical">Practical Notes</a></li>
    </ol>
</div>

<h3>Terminology</h3>

<table style="font-size: 0.82em; line-height: 1.4;">
    <tr><th>Term</th><th>Full Form</th><th>What It Is</th></tr>
    <tr><td><strong>PE / DE</strong></td><td>Prefill Engine / Decode Engine</td><td>PE processes the full prompt in parallel (compute-heavy); DE generates tokens autoregressively (memory-heavy).</td></tr>
    <tr><td><strong>PD</strong></td><td>Prefill-Decode</td><td>Disaggregated serving: prefill and decode on separate GPU pools.</td></tr>
    <tr><td><strong>KV-Cache</strong></td><td>Key-Value Cache</td><td>Cached attention tensors from previous tokens, reused across turns.</td></tr>
    <tr><td><strong>NIC / CNIC</strong></td><td>Network Interface Card / Compute NIC</td><td>NIC connects a server to a network. CNIC is the high-bandwidth NIC for inter-GPU collectives (AllToAll, ReduceScatter).</td></tr>
    <tr><td><strong>RDMA</strong></td><td>Remote Direct Memory Access</td><td>One machine reads/writes another's memory directly, bypassing the CPU.</td></tr>
    <tr><td><strong>HBM / DRAM</strong></td><td>High Bandwidth Memory / Dynamic RAM</td><td>HBM is GPU on-chip memory; DRAM is CPU-side host memory used as a staging buffer.</td></tr>
    <tr><td><strong>PCIe</strong></td><td>Peripheral Component Interconnect Express</td><td>High-speed bus connecting GPUs, NICs, and other devices in a server.</td></tr>
    <tr><td><strong>3FS</strong></td><td>Fire-Flyer File System</td><td>DeepSeek's distributed filesystem for persistent KV-Cache storage.</td></tr>
    <tr><td><strong>QoS / VL</strong></td><td>Quality of Service / Virtual Lane</td><td>QoS prioritizes traffic. VLs are hardware channels in an InfiniBand link for independent flow control.</td></tr>
    <tr><td><strong>RoCE</strong></td><td>RDMA over Converged Ethernet</td><td>RDMA on standard Ethernet (alternative to InfiniBand).</td></tr>
    <tr><td><strong>H2D / D2H</strong></td><td>Host-to-Device / Device-to-Host</td><td>Memory transfers between CPU DRAM and GPU HBM.</td></tr>
    <tr><td><strong>JCT</strong></td><td>Job Completion Time</td><td>Wall-clock time from request submission to full response.</td></tr>
    <tr><td><strong>TTFT / TTST</strong></td><td>Time to First Token / Time to Second Token</td><td>TTFT: latency before first output. TTST: proxy for per-token decode latency.</td></tr>
    <tr><td><strong>FLOPS</strong></td><td>Floating Point Ops Per Second</td><td>GPU compute throughput. PFLOP = 10<sup>15</sup> FLOP.</td></tr>
</table>

<!-- ============================================================ -->
<h2 id="problem">1. The Problem: Storage NICs Can't Keep Up</h2>

<p>Modern LLM serving uses <strong>prefill-decode (PD) disaggregation</strong>: prefill and decode run on separate GPU pools. For agentic workloads (multi-turn tool-using agents), the context grows across turns, and nearly all of it is reusable via KV-Cache. DeepSeek reports a 98.7% KV-Cache hit rate in their agentic RL training workloads.</p>

<p>This sounds like good news, but it creates a bottleneck. All that cached KV data lives on remote storage (like 3FS) and must be loaded into GPU memory before prefill can begin. The storage NICs on the prefill side become the chokepoint.</p>

<ul>
    <li><strong>GPU compute is not the bottleneck.</strong> For DeepSeek-V3.2 with a 98.7% hit rate, the cache-compute ratio is ~22 GB/PFLOP. The GPUs are waiting for data, not the other way around.</li>
    <li><strong>Hardware trends make it worse.</strong> From Ampere to Blackwell, GPU FLOPS grew 28.8x but NIC bandwidth only grew 2.0x. The I/O-to-compute ratio has dropped 14.4x.</li>
    <li><strong>Bandwidth is wasted.</strong> In conventional PD systems, only prefill engines read KV-Cache from storage. Decode engines' storage NICs sit idle. Half the cluster's storage bandwidth goes unused.</li>
</ul>

<!-- ============================================================ -->
<h2 id="background">2. Background: PD Disaggregation and Agentic Workloads</h2>

<h3>Prefill-Decode Disaggregation</h3>

<p>In PD-disaggregated inference, the cluster is split into two pools of GPUs:</p>

<ul>
    <li><strong>Prefill Engines (PEs)</strong> process the full prompt (all input tokens in parallel). This is compute-heavy.</li>
    <li><strong>Decode Engines (DEs)</strong> generate tokens one at a time autoregressively. This is memory-bandwidth-heavy.</li>
</ul>

<p>After prefill completes, the KV-Cache is transferred from the PE to a DE, which then handles decoding. Separating the two stages lets you optimize hardware and batching independently for each.</p>

<h3>Why Agentic Workloads Are Special</h3>

<p>An agentic workload is a multi-turn conversation where the LLM calls tools, reads outputs, and continues reasoning. Each turn appends new tokens (tool call + tool result) to a growing context.</p>

<p>The critical property: each new turn only adds a few hundred tokens to a context of tens of thousands. So 98%+ of the KV-Cache from previous turns can be reused. This cache is stored in distributed storage (3FS) and must be loaded before each prefill.</p>

<div class="diagram">
    <svg viewBox="0 0 500 290" xmlns="http://www.w3.org/2000/svg" font-family="Georgia, serif">
        <defs>
            <filter id="shadow"><feDropShadow dx="0" dy="1" stdDeviation="2" flood-opacity="0.08"/></filter>
        </defs>

        <text x="250" y="18" text-anchor="middle" font-size="13" font-weight="bold" fill="#333">Agentic Turn Structure</text>

        <!-- Turn N context bar -->
        <rect x="30" y="35" width="440" height="32" rx="4" fill="#f9fafb" stroke="#bbb" filter="url(#shadow)"/>
        <text x="250" y="55" text-anchor="middle" font-size="11" fill="#555">Turn N context: 30,000 tokens</text>

        <!-- Breakdown -->
        <rect x="30" y="80" width="390" height="28" rx="4" fill="#f0fdf4" stroke="#16a34a"/>
        <text x="225" y="98" text-anchor="middle" font-size="11" fill="#166534">Cached KV from previous turns (~29,400 tokens, 98%)</text>

        <rect x="422" y="80" width="48" height="28" rx="4" fill="#f0f4ff" stroke="#2563eb"/>
        <text x="446" y="98" text-anchor="middle" font-size="9" fill="#1e40af">New</text>

        <!-- Arrow: cached portion -> storage -->
        <path d="M 225 108 L 225 148" fill="none" stroke="#16a34a" stroke-width="2"/>
        <polygon points="225,148 220,138 230,138" fill="#16a34a"/>
        <text x="320" y="134" font-size="10" font-style="italic" fill="#555">Load from storage</text>

        <!-- Storage -->
        <rect x="100" y="152" width="250" height="34" rx="6" fill="#f9fafb" stroke="#bbb" filter="url(#shadow)"/>
        <text x="225" y="173" text-anchor="middle" font-size="11" fill="#333">Distributed Storage (3FS)</text>

        <!-- Arrow: storage -> PE GPU -->
        <path d="M 225 186 L 225 222" fill="none" stroke="#888" stroke-width="2"/>
        <polygon points="225,222 220,212 230,212" fill="#888"/>
        <text x="320" y="208" font-size="10" font-style="italic" fill="#555">Via storage NIC</text>

        <!-- PE -->
        <rect x="140" y="224" width="170" height="34" rx="6" fill="#f0f4ff" stroke="#2563eb" filter="url(#shadow)"/>
        <text x="225" y="245" text-anchor="middle" font-size="11" font-weight="bold" fill="#1e40af">Prefill Engine GPU</text>

        <!-- Bottleneck annotation -->
        <rect x="30" y="268" width="440" height="20" rx="3" fill="#fff5f5" stroke="#dc2626" stroke-dasharray="4,3"/>
        <text x="250" y="282" text-anchor="middle" font-size="10" fill="#991b1b">Bottleneck: storage NIC bandwidth limits how fast KV-Cache reaches the PE</text>
    </svg>
</div>

<div class="note">
    <strong>Intuition:</strong> Think of a restaurant kitchen with two stations: a prep station (PE) that assembles ingredients and a cooking station (DE) that cooks dishes. Both stations have their own loading docks for receiving deliveries from the warehouse. But right now, only the prep station ever orders from the warehouse. The cooking station's loading dock sits empty. Meanwhile, the prep station's dock is jammed with delivery trucks. DualPath lets the cooking station receive deliveries too, and pass ingredients to the prep station through an internal service window that's barely used during meal service.
</div>

<!-- ============================================================ -->
<h2 id="insight">3. The Key Insight</h2>

<p>The observation that makes DualPath work is simple: the cluster has two separate networks, and they have very different utilization patterns.</p>

<table>
    <thead>
        <tr>
            <th>Network</th>
            <th>Purpose</th>
            <th>Bandwidth</th>
            <th>Utilization Pattern</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><strong>Storage network</strong></td>
            <td>KV-Cache read/write to 3FS</td>
            <td>~50 GB/s per node</td>
            <td>Saturated on PE side, idle on DE side</td>
        </tr>
        <tr>
            <td><strong>Compute network</strong></td>
            <td>AllToAll, ReduceScatter (model parallelism)</td>
            <td>~400 Gbps per NIC (8 NICs/node)</td>
            <td>Bursty: sub-ms bursts with idle gaps</td>
        </tr>
    </tbody>
</table>

<p>The compute network (InfiniBand RDMA) has far higher bandwidth: each node has 8 CNICs at 400 Gbps each (~400 GB/s aggregate) versus a single storage NIC at ~50 GB/s. This is by design; model parallelism requires moving large activation tensors between GPUs every forward pass, so the compute fabric is provisioned for peak throughput. But collective operations (AllToAll, ReduceScatter) happen in short bursts with idle gaps between them, leaving most of that bandwidth unused most of the time. Meanwhile, decode engines have their own storage NICs that do nothing during prefill-heavy phases.</p>

<p>DualPath's idea: load KV-Cache through <em>both</em> prefill and decode engines' storage NICs, then use the high-bandwidth compute network to shuttle data from decode engines to prefill engines. This transforms storage I/O from a single-sided bottleneck into a distributed, schedulable resource across all nodes.</p>

<!-- ============================================================ -->
<h2 id="architecture">4. DualPath Architecture</h2>

<div class="diagram">
    <svg viewBox="0 0 510 540" xmlns="http://www.w3.org/2000/svg" font-family="Georgia, serif">
        <defs>
            <filter id="sh"><feDropShadow dx="0" dy="1" stdDeviation="2" flood-opacity="0.08"/></filter>
        </defs>

        <text x="255" y="20" text-anchor="middle" font-size="13" font-weight="bold" fill="#333">DualPath: Two Paths for KV-Cache Loading</text>

        <!-- ===== PE Node ===== -->
        <rect x="20" y="42" width="210" height="280" rx="10" fill="#fcfff8" stroke="#16a34a" stroke-width="1.5" stroke-dasharray="6,3"/>
        <text x="125" y="62" text-anchor="middle" font-size="11" font-weight="bold" fill="#166534">Prefill Engine Node</text>

        <rect x="45" y="76" width="160" height="36" rx="6" fill="#f0f4ff" stroke="#2563eb" filter="url(#sh)"/>
        <text x="125" y="99" text-anchor="middle" font-size="12" fill="#1e40af">GPU (HBM)</text>

        <rect x="45" y="148" width="160" height="36" rx="6" fill="#f9fafb" stroke="#bbb" filter="url(#sh)"/>
        <text x="125" y="171" text-anchor="middle" font-size="12" fill="#333">CNIC</text>

        <rect x="45" y="220" width="160" height="36" rx="6" fill="#f9fafb" stroke="#bbb" filter="url(#sh)"/>
        <text x="125" y="243" text-anchor="middle" font-size="12" fill="#333">Buffer (DRAM)</text>

        <rect x="45" y="282" width="160" height="36" rx="6" fill="#f9fafb" stroke="#bbb" filter="url(#sh)"/>
        <text x="125" y="305" text-anchor="middle" font-size="12" fill="#333">Storage NIC</text>

        <!-- ===== DE Node ===== -->
        <rect x="280" y="42" width="210" height="280" rx="10" fill="#fffcf5" stroke="#d97706" stroke-width="1.5" stroke-dasharray="6,3"/>
        <text x="385" y="62" text-anchor="middle" font-size="11" font-weight="bold" fill="#92400e">Decode Engine Node</text>

        <rect x="305" y="76" width="160" height="36" rx="6" fill="#f9fafb" stroke="#ccc" stroke-dasharray="3,2" filter="url(#sh)"/>
        <text x="385" y="96" text-anchor="middle" font-size="11" fill="#aaa">GPU (HBM)</text>
        <text x="385" y="108" text-anchor="middle" font-size="8" fill="#bbb">idle during KV load</text>

        <rect x="305" y="148" width="160" height="36" rx="6" fill="#f9fafb" stroke="#bbb" filter="url(#sh)"/>
        <text x="385" y="171" text-anchor="middle" font-size="12" fill="#333">CNIC</text>

        <rect x="305" y="220" width="160" height="36" rx="6" fill="#f9fafb" stroke="#bbb" filter="url(#sh)"/>
        <text x="385" y="243" text-anchor="middle" font-size="12" fill="#333">Buffer (DRAM)</text>

        <rect x="305" y="282" width="160" height="36" rx="6" fill="#f9fafb" stroke="#bbb" filter="url(#sh)"/>
        <text x="385" y="305" text-anchor="middle" font-size="12" fill="#333">Storage NIC</text>

        <!-- ===== Storage ===== -->
        <rect x="115" y="470" width="280" height="40" rx="8" fill="#f9fafb" stroke="#bbb" filter="url(#sh)"/>
        <text x="255" y="495" text-anchor="middle" font-size="12" fill="#333">Persistent Storage (3FS)</text>

        <!-- ===== PATH 1: PE Read Path (green, left side) ===== -->
        <!-- Full continuous path drawn as a polyline for clarity -->
        <!-- Storage -> PE SNIC: vertical green line on left -->
        <path d="M 100 470 L 100 318" fill="none" stroke="#16a34a" stroke-width="2.5"/>
        <polygon points="100,318 95,328 105,328" fill="#16a34a"/>

        <!-- PE SNIC -> PE Buffer -->
        <path d="M 100 282 L 100 256" fill="none" stroke="#16a34a" stroke-width="2.5"/>
        <polygon points="100,256 95,266 105,266" fill="#16a34a"/>

        <!-- PE Buffer -> PE CNIC -->
        <path d="M 100 220 L 100 184" fill="none" stroke="#16a34a" stroke-width="2.5"/>
        <polygon points="100,184 95,194 105,194" fill="#16a34a"/>

        <!-- PE CNIC -> PE GPU -->
        <path d="M 100 148 L 100 112" fill="none" stroke="#16a34a" stroke-width="2.5"/>
        <polygon points="100,112 95,122 105,122" fill="#16a34a"/>

        <!-- PE path label -->
        <rect x="4" y="384" width="90" height="42" rx="5" fill="#f0fdf4" stroke="#16a34a"/>
        <text x="49" y="402" text-anchor="middle" font-size="10" font-weight="bold" fill="#166534">PE Read Path</text>
        <text x="49" y="418" text-anchor="middle" font-size="9" fill="#166534">(conventional)</text>

        <!-- ===== PATH 2: DE Read Path (blue, right side) ===== -->
        <!-- Storage -> DE SNIC -->
        <path d="M 410 470 L 410 318" fill="none" stroke="#2563eb" stroke-width="2.5"/>
        <polygon points="410,318 405,328 415,328" fill="#2563eb"/>

        <!-- DE SNIC -> DE Buffer -->
        <path d="M 410 282 L 410 256" fill="none" stroke="#2563eb" stroke-width="2.5"/>
        <polygon points="410,256 405,266 415,266" fill="#2563eb"/>

        <!-- DE Buffer -> DE CNIC -->
        <path d="M 410 220 L 410 184" fill="none" stroke="#2563eb" stroke-width="2.5"/>
        <polygon points="410,184 405,194 415,194" fill="#2563eb"/>

        <!-- DE CNIC -> PE CNIC (horizontal RDMA transfer) -->
        <path d="M 305 166 L 207 166" fill="none" stroke="#2563eb" stroke-width="2.5"/>
        <polygon points="207,166 217,161 217,171" fill="#2563eb"/>

        <!-- RDMA label above the horizontal arrow -->
        <rect x="216" y="140" width="80" height="18" rx="4" fill="#f0f4ff" stroke="#2563eb" stroke-width="0.8"/>
        <text x="256" y="153" text-anchor="middle" font-size="9" font-weight="bold" fill="#1e40af">RDMA</text>

        <!-- PE CNIC -> PE GPU (completing the blue path on PE side) -->
        <path d="M 150 148 L 150 112" fill="none" stroke="#2563eb" stroke-width="2.5"/>
        <polygon points="150,112 145,122 155,122" fill="#2563eb"/>

        <!-- DE path label -->
        <rect x="416" y="384" width="85" height="42" rx="5" fill="#f0f4ff" stroke="#2563eb"/>
        <text x="459" y="402" text-anchor="middle" font-size="10" font-weight="bold" fill="#1e40af">DE Read Path</text>
        <text x="459" y="418" text-anchor="middle" font-size="9" fill="#1e40af">(new)</text>

        <!-- ===== Scheduler ===== -->
        <rect x="185" y="350" width="140" height="32" rx="6" fill="#fffbeb" stroke="#d97706" filter="url(#sh)"/>
        <text x="255" y="371" text-anchor="middle" font-size="11" font-weight="bold" fill="#92400e">Request Scheduler</text>

        <!-- Scheduler dashed arrows -->
        <path d="M 210 350 L 125 325" fill="none" stroke="#888" stroke-width="1" stroke-dasharray="4,3"/>
        <path d="M 300 350 L 385 325" fill="none" stroke="#888" stroke-width="1" stroke-dasharray="4,3"/>

        <text x="255" y="530" text-anchor="middle" font-size="10" font-style="italic" fill="#555">Scheduler dynamically splits KV-Cache load across both paths</text>
    </svg>
</div>

<h3>Animated: Baseline vs. DualPath</h3>

<p>This animation shows a real scenario: Agent Turn 15 of an RL training rollout. The context is 32K tokens, 98.7% cached (31.6K tokens in KV-Cache on storage). Both systems must load the same amount of cached data before prefill can begin. Click "Start" to compare.</p>

<div class="diagram" style="padding: 12px 12px 4px; overflow: visible;">
    <div id="anim-container" style="position: relative;">
        <canvas id="anim-canvas" width="960" height="780" style="width: 100%; height: auto;"></canvas>
    </div>
    <div style="text-align:center; margin: 8px 0 2px;">
        <button id="anim-btn" style="
            font-family: Georgia, serif;
            font-size: 14px;
            padding: 7px 28px;
            background: #f0f4ff;
            color: #1e40af;
            border: 1.5px solid #2563eb;
            border-radius: 6px;
            cursor: pointer;
        ">Start</button>
    </div>
</div>

<script>
(function() {
    var canvas = document.getElementById('anim-canvas');
    var ctx = canvas.getContext('2d');
    var btn = document.getElementById('anim-btn');
    var W = canvas.width, H = canvas.height;
    var running = false;
    var frame = 0;
    var animId = null;

    // Layout constants
    var COL_W = 440;
    var GAP = 80;
    var LEFT_X = 0;
    var RIGHT_X = LEFT_X + COL_W + GAP;
    var SCENE_TOP = 130;

    // Scenario info
    var TOTAL_BLOCKS = 16; // KV-Cache blocks to load
    var BLOCK_SPEED_BASE = 2.4; // pixels per frame for baseline
    var BLOCK_SPEED_DUAL = 2.4; // same speed per NIC, but two NICs

    // Component positions (relative to column x)
    var GPU_Y = SCENE_TOP + 14;
    var CNIC_Y = SCENE_TOP + 100;
    var BUF_Y = SCENE_TOP + 186;
    var NIC_Y = SCENE_TOP + 272;
    var STOR_Y = SCENE_TOP + 375;
    var BOX_W = 180;
    var BOX_H = 44;
    var DE_OFFSET = 220; // DE column offset within right panel

    // State
    var baselineDelivered = 0;
    var dualDelivered = 0;
    var baselinePackets = [];
    var dualPackets = [];
    var baselineDone = false;
    var dualDone = false;
    var baselineTime = 0;
    var dualTime = 0;
    var baselineSpawnTimer = 0;
    var dualSpawnTimer = 0;
    var phase = 'idle'; // idle, running, done
    var annotations = [];
    var annotAlpha = {};

    // DE components for DualPath right panel
    var DE_GPU_Y = GPU_Y;
    var DE_CNIC_Y = CNIC_Y;
    var DE_BUF_Y = BUF_Y;
    var DE_NIC_Y = NIC_Y;

    function roundRect(x, y, w, h, r, fill, stroke, dash) {
        ctx.beginPath();
        ctx.moveTo(x + r, y);
        ctx.lineTo(x + w - r, y);
        ctx.arcTo(x + w, y, x + w, y + r, r);
        ctx.lineTo(x + w, y + h - r);
        ctx.arcTo(x + w, y + h, x + w - r, y + h, r);
        ctx.lineTo(x + r, y + h);
        ctx.arcTo(x, y + h, x, y + h - r, r);
        ctx.lineTo(x, y + r);
        ctx.arcTo(x, y, x + r, y, r);
        ctx.closePath();
        if (dash) ctx.setLineDash(dash);
        if (fill) { ctx.fillStyle = fill; ctx.fill(); }
        if (stroke) { ctx.strokeStyle = stroke; ctx.lineWidth = 1.5; ctx.stroke(); }
        ctx.setLineDash([]);
    }

    function text(str, x, y, size, color, weight, align) {
        ctx.font = (weight || 'normal') + ' ' + (size || 12) + 'px Georgia, serif';
        ctx.fillStyle = color || '#333';
        ctx.textAlign = align || 'center';
        ctx.textBaseline = 'middle';
        ctx.fillText(str, x, y);
    }

    function drawArrow(x1, y1, x2, y2, color, width) {
        var dx = x2 - x1, dy = y2 - y1;
        var len = Math.sqrt(dx*dx + dy*dy);
        var ux = dx/len, uy = dy/len;
        ctx.beginPath();
        ctx.moveTo(x1, y1);
        ctx.lineTo(x2, y2);
        ctx.strokeStyle = color;
        ctx.lineWidth = width || 1.5;
        ctx.stroke();
        // arrowhead
        var hs = 6;
        ctx.beginPath();
        ctx.moveTo(x2, y2);
        ctx.lineTo(x2 - ux*hs - uy*hs*0.5, y2 - uy*hs + ux*hs*0.5);
        ctx.lineTo(x2 - ux*hs + uy*hs*0.5, y2 - uy*hs - ux*hs*0.5);
        ctx.closePath();
        ctx.fillStyle = color;
        ctx.fill();
    }

    // Draw a single-column pipeline (baseline)
    function drawPipeline(cx, label, isBaseline) {
        var bx = cx - BOX_W/2;

        // Column label
        text(label, cx, SCENE_TOP - 18, 18, '#333', 'bold');

        // GPU
        roundRect(bx, GPU_Y, BOX_W, BOX_H, 8, '#f0f4ff', '#2563eb');
        text('PE GPU (HBM)', cx, GPU_Y + BOX_H/2, 15, '#1e40af', 'bold');

        // CNIC
        roundRect(bx, CNIC_Y, BOX_W, BOX_H, 8, '#f9fafb', '#bbb');
        text('PE CNIC', cx, CNIC_Y + BOX_H/2, 15, '#333');

        // Buffer
        roundRect(bx, BUF_Y, BOX_W, BOX_H, 8, '#f9fafb', '#bbb');
        text('PE Buffer', cx, BUF_Y + BOX_H/2, 15, '#333');

        // NIC
        roundRect(bx, NIC_Y, BOX_W, BOX_H, 8, '#f9fafb', '#bbb');
        text('PE Storage NIC', cx, NIC_Y + BOX_H/2, 15, '#333');

        // Storage
        roundRect(bx - 14, STOR_Y, BOX_W + 28, BOX_H + 6, 8, '#f9fafb', '#bbb');
        text('Storage (3FS)', cx, STOR_Y + (BOX_H+6)/2, 15, '#555');

        // Faint path line
        ctx.beginPath();
        ctx.moveTo(cx, STOR_Y);
        ctx.lineTo(cx, GPU_Y + BOX_H);
        ctx.strokeStyle = 'rgba(22,163,106,0.15)';
        ctx.lineWidth = 2;
        ctx.stroke();
    }

    // Draw the DualPath dual-column pipeline
    function drawDualPipeline(panelX) {
        var peCx = panelX + 100;
        var deCx = panelX + 100 + DE_OFFSET;
        var peBx = peCx - BOX_W/2;
        var deBx = deCx - BOX_W/2 + 10;
        var deBoxW = BOX_W - 20;

        text('DualPath', panelX + 100 + DE_OFFSET/2, SCENE_TOP - 18, 18, '#333', 'bold');

        // PE side
        roundRect(peBx, GPU_Y, BOX_W, BOX_H, 8, '#f0f4ff', '#2563eb');
        text('PE GPU (HBM)', peCx, GPU_Y + BOX_H/2, 15, '#1e40af', 'bold');

        roundRect(peBx, CNIC_Y, BOX_W, BOX_H, 8, '#f9fafb', '#bbb');
        text('PE CNIC', peCx, CNIC_Y + BOX_H/2, 15, '#333');

        roundRect(peBx, BUF_Y, BOX_W, BOX_H, 8, '#f9fafb', '#bbb');
        text('PE Buffer', peCx, BUF_Y + BOX_H/2, 15, '#333');

        roundRect(peBx, NIC_Y, BOX_W, BOX_H, 8, '#f9fafb', '#bbb');
        text('PE Storage NIC', peCx, NIC_Y + BOX_H/2, 15, '#333');

        // DE side (smaller)
        roundRect(deBx, DE_CNIC_Y, deBoxW, BOX_H, 8, '#f9fafb', '#bbb');
        text('DE CNIC', deCx, DE_CNIC_Y + BOX_H/2, 14, '#333');

        roundRect(deBx, DE_BUF_Y, deBoxW, BOX_H, 8, '#f9fafb', '#bbb');
        text('DE Buffer', deCx, DE_BUF_Y + BOX_H/2, 14, '#333');

        roundRect(deBx, DE_NIC_Y, deBoxW, BOX_H, 8, '#f9fafb', '#bbb');
        text('DE Storage NIC', deCx, DE_NIC_Y + BOX_H/2, 14, '#333');

        // Storage (shared, wide)
        roundRect(peBx - 14, STOR_Y, DE_OFFSET + BOX_W + 18, BOX_H + 6, 8, '#f9fafb', '#bbb');
        text('Storage (3FS)', peCx + DE_OFFSET/2, STOR_Y + (BOX_H+6)/2, 15, '#555');

        // Faint path lines
        // Green (PE path)
        ctx.beginPath();
        ctx.moveTo(peCx, STOR_Y);
        ctx.lineTo(peCx, GPU_Y + BOX_H);
        ctx.strokeStyle = 'rgba(22,163,106,0.15)';
        ctx.lineWidth = 2;
        ctx.stroke();

        // Blue (DE path: up DE side, across, up PE side)
        ctx.beginPath();
        ctx.moveTo(deCx, STOR_Y);
        ctx.lineTo(deCx, DE_CNIC_Y + BOX_H/2);
        ctx.lineTo(peCx + BOX_W/2, CNIC_Y + BOX_H/2);
        ctx.strokeStyle = 'rgba(37,99,235,0.15)';
        ctx.lineWidth = 2;
        ctx.stroke();
        ctx.beginPath();
        ctx.moveTo(peCx + 15, CNIC_Y + BOX_H/2);
        ctx.lineTo(peCx + 15, GPU_Y + BOX_H);
        ctx.strokeStyle = 'rgba(37,99,235,0.15)';
        ctx.lineWidth = 2;
        ctx.stroke();

        // RDMA label
        var mx = (deCx + peCx + BOX_W/2) / 2;
        var my = CNIC_Y + BOX_H/2 - 16;
        roundRect(mx - 34, my - 10, 68, 20, 4, '#f0f4ff', '#2563eb');
        text('RDMA', mx, my, 12, '#1e40af', 'bold');
    }

    // Packet: {x, y, path (array of {x,y}), seg, t, speed, color, done}
    function createPkt(pathPts, color, speed) {
        return {
            x: pathPts[0].x, y: pathPts[0].y,
            path: pathPts, seg: 0, t: 0,
            speed: speed, color: color, done: false,
        };
    }

    function updatePkt(p) {
        if (p.done) return;
        var a = p.path[p.seg];
        var b = p.path[p.seg + 1];
        var dx = b.x - a.x, dy = b.y - a.y;
        var segLen = Math.sqrt(dx*dx + dy*dy);
        p.t += p.speed / segLen;
        if (p.t >= 1) {
            p.t = 0;
            p.seg++;
            if (p.seg >= p.path.length - 1) {
                p.done = true;
                return;
            }
        }
        var aa = p.path[p.seg];
        var bb = p.path[p.seg + 1];
        p.x = aa.x + (bb.x - aa.x) * p.t;
        p.y = aa.y + (bb.y - aa.y) * p.t;
    }

    function drawPkt(p) {
        if (p.done) return;
        var isGreen = p.color === 'green';
        var fill = isGreen ? '#dcfce7' : '#dbeafe';
        var stroke = isGreen ? '#16a34a' : '#2563eb';
        var txtC = isGreen ? '#166534' : '#1e40af';
        roundRect(p.x - 14, p.y - 10, 28, 20, 4, fill, stroke);
        text('KV', p.x, p.y, 11, txtC, 'bold');
    }

    // Context bar at top
    function drawContextBar() {
        var y = 20;
        var bw = W - 60;
        roundRect(30, y, bw, 36, 6, '#f9fafb', '#bbb');
        // Cached portion (98.7%)
        var cw = bw * 0.987;
        roundRect(30, y, cw, 36, 6, '#f0fdf4', '#16a34a');
        text('31.6K tokens cached (98.7%)', 30 + cw/2, y + 18, 14, '#166534', 'bold');
        // New portion
        roundRect(30 + cw, y, bw - cw, 36, 0, '#f0f4ff', '#2563eb');
        text('New', 30 + cw + (bw - cw)/2, y + 18, 11, '#1e40af', 'bold');
        // Label
        text('Agent Turn 15  |  Context: 32K tokens', W/2, y + 56, 15, '#555', 'normal');
    }

    // Progress bar
    function drawProgress(cx, y, delivered, total, elapsed, done) {
        var bw = 170;
        var bh = 20;
        roundRect(cx - bw/2, y, bw, bh, 5, '#eee', '#ccc');
        var fillW = bw * Math.min(delivered / total, 1);
        if (fillW > 0) {
            roundRect(cx - bw/2, y, fillW, bh, 5, done ? '#dcfce7' : '#dbeafe', done ? '#16a34a' : '#2563eb');
        }
        text(delivered + '/' + total + ' blocks', cx, y + bh + 16, 14, '#555');
        // Timer
        var secs = (elapsed / 60).toFixed(1);
        text(secs + 's', cx, y + bh + 36, 15, done ? '#16a34a' : '#1e40af', 'bold');
    }

    // Annotation bubble
    function drawAnnotation(str, x, y, alpha) {
        if (alpha <= 0) return;
        ctx.globalAlpha = alpha;
        var tw = ctx.measureText ? 14 * str.length * 0.55 : 220;
        var bw = Math.max(tw + 24, 180);
        roundRect(x - bw/2, y - 14, bw, 28, 6, '#fffbeb', '#d97706');
        text(str, x, y, 13, '#92400e', 'bold');
        ctx.globalAlpha = 1;
    }

    // Baseline green path waypoints
    function baselineGreenPath(cx) {
        return [
            {x: cx, y: STOR_Y},
            {x: cx, y: NIC_Y + BOX_H},
            {x: cx, y: NIC_Y},
            {x: cx, y: BUF_Y + BOX_H},
            {x: cx, y: BUF_Y},
            {x: cx, y: CNIC_Y + BOX_H},
            {x: cx, y: CNIC_Y},
            {x: cx, y: GPU_Y + BOX_H},
        ];
    }

    // DualPath green path (PE side)
    function dualGreenPath(peCx) {
        return baselineGreenPath(peCx);
    }

    // DualPath blue path (DE side -> RDMA -> PE GPU)
    function dualBluePath(peCx, deCx) {
        return [
            {x: deCx, y: STOR_Y},
            {x: deCx, y: DE_NIC_Y + BOX_H},
            {x: deCx, y: DE_NIC_Y},
            {x: deCx, y: DE_BUF_Y + BOX_H},
            {x: deCx, y: DE_BUF_Y},
            {x: deCx, y: DE_CNIC_Y + BOX_H},
            {x: deCx, y: DE_CNIC_Y + BOX_H/2},
            {x: peCx + BOX_W/2, y: CNIC_Y + BOX_H/2}, // RDMA to PE CNIC
            {x: peCx + 15, y: CNIC_Y + BOX_H/2},
            {x: peCx + 15, y: CNIC_Y},
            {x: peCx + 15, y: GPU_Y + BOX_H},
        ];
    }

    // Column centers
    var baseCx = LEFT_X + COL_W/2;
    var dualPeCx = RIGHT_X + 100;
    var dualDeCx = RIGHT_X + 100 + DE_OFFSET;

    var baseGreen = baselineGreenPath(baseCx);
    var dGreen = dualGreenPath(dualPeCx);
    var dBlue = dualBluePath(dualPeCx, dualDeCx);

    var SPAWN_INTERVAL = 24; // frames between spawns

    function reset() {
        frame = 0;
        baselineDelivered = 0;
        dualDelivered = 0;
        baselinePackets = [];
        dualPackets = [];
        baselineDone = false;
        dualDone = false;
        baselineTime = 0;
        dualTime = 0;
        baselineSpawnTimer = 0;
        dualSpawnTimer = 0;
        annotations = [];
        annotAlpha = {};
    }

    function spawnAnnotation(key, str, x, y, atFrame) {
        if (frame === atFrame && !annotAlpha[key]) {
            annotations.push({key:key, str:str, x:x, y:y, birth:frame});
            annotAlpha[key] = 1;
        }
    }

    function draw() {
        ctx.clearRect(0, 0, W, H);

        // Context bar
        drawContextBar();

        // Divider
        ctx.beginPath();
        ctx.moveTo(COL_W + GAP/2, SCENE_TOP - 36);
        ctx.lineTo(COL_W + GAP/2, H - 20);
        ctx.strokeStyle = '#e0e0e0';
        ctx.lineWidth = 1;
        ctx.setLineDash([4,4]);
        ctx.stroke();
        ctx.setLineDash([]);

        // Draw pipelines
        drawPipeline(baseCx, 'Baseline', true);
        drawDualPipeline(RIGHT_X);

        // Progress bars
        var progY = STOR_Y + BOX_H + 30;
        drawProgress(baseCx, progY, baselineDelivered, TOTAL_BLOCKS, baselineTime, baselineDone);
        drawProgress(RIGHT_X + 100 + DE_OFFSET/2, progY, dualDelivered, TOTAL_BLOCKS, dualTime, dualDone);

        // Draw packets
        for (var i = 0; i < baselinePackets.length; i++) drawPkt(baselinePackets[i]);
        for (var i = 0; i < dualPackets.length; i++) drawPkt(dualPackets[i]);

        // Annotations
        for (var i = 0; i < annotations.length; i++) {
            var a = annotations[i];
            var age = frame - a.birth;
            var alpha = age < 10 ? age/10 : (age > 120 ? Math.max(0, 1-(age-120)/30) : 1);
            drawAnnotation(a.str, a.x, a.y, alpha);
        }

        // "NIC saturated" on baseline
        if (phase === 'running' && !baselineDone && frame > 60) {
            ctx.globalAlpha = 0.6 + 0.4 * Math.sin(frame * 0.1);
            roundRect(baseCx - 70, NIC_Y - 22, 140, 20, 4, '#fff5f5', '#dc2626');
            text('NIC saturated', baseCx, NIC_Y - 12, 12, '#991b1b', 'bold');
            ctx.globalAlpha = 1;
        }

        // DE NIC "idle" on baseline
        if (phase === 'running' && !baselineDone) {
            text('DE NIC idle', baseCx, NIC_Y + BOX_H + 22, 13, '#aaa', 'normal');
        }

        // Result comparison
        if (baselineDone && dualDone) {
            var speedup = (baselineTime / dualTime).toFixed(1);
            var ry = progY + 72;
            roundRect(W/2 - 200, ry, 400, 44, 8, '#f0fdf4', '#16a34a');
            text('DualPath finished ' + speedup + 'x faster by using both NICs', W/2, ry + 22, 15, '#166534', 'bold');
        }
    }

    function tick() {
        if (phase !== 'running') return;
        frame++;

        // Timed annotations
        spawnAnnotation('load', 'Loading 31.6K tokens of KV-Cache...', W/2, SCENE_TOP - 40, 10);
        spawnAnnotation('pe_only', 'Only 1 NIC path available', baseCx, STOR_Y - 16, 50);
        spawnAnnotation('de_help', 'DE NIC helps load in parallel', dualDeCx, STOR_Y - 16, 50);

        // Baseline: spawn green packets one at a time
        if (!baselineDone) {
            baselineTime++;
            baselineSpawnTimer++;
            if (baselineSpawnTimer >= SPAWN_INTERVAL && baselineDelivered + baselinePackets.length < TOTAL_BLOCKS) {
                baselinePackets.push(createPkt(baseGreen, 'green', BLOCK_SPEED_BASE));
                baselineSpawnTimer = 0;
            }
            for (var i = baselinePackets.length - 1; i >= 0; i--) {
                updatePkt(baselinePackets[i]);
                if (baselinePackets[i].done) {
                    baselinePackets.splice(i, 1);
                    baselineDelivered++;
                }
            }
            if (baselineDelivered >= TOTAL_BLOCKS) baselineDone = true;
        }

        // DualPath: spawn green and blue alternately
        if (!dualDone) {
            dualTime++;
            dualSpawnTimer++;
            if (dualSpawnTimer >= SPAWN_INTERVAL && dualDelivered + dualPackets.length < TOTAL_BLOCKS) {
                // Alternate green and blue
                var useBlue = (dualDelivered + dualPackets.length) % 2 === 1;
                if (useBlue) {
                    dualPackets.push(createPkt(dBlue, 'blue', BLOCK_SPEED_DUAL));
                } else {
                    dualPackets.push(createPkt(dGreen, 'green', BLOCK_SPEED_DUAL));
                }
                dualSpawnTimer = 0;
            }
            // Spawn the other color right away (parallel loading)
            if (dualSpawnTimer === Math.floor(SPAWN_INTERVAL/2) && dualDelivered + dualPackets.length < TOTAL_BLOCKS) {
                var useBlue2 = (dualDelivered + dualPackets.length) % 2 === 1;
                if (useBlue2) {
                    dualPackets.push(createPkt(dBlue, 'blue', BLOCK_SPEED_DUAL));
                } else {
                    dualPackets.push(createPkt(dGreen, 'green', BLOCK_SPEED_DUAL));
                }
            }
            for (var i = dualPackets.length - 1; i >= 0; i--) {
                updatePkt(dualPackets[i]);
                if (dualPackets[i].done) {
                    dualPackets.splice(i, 1);
                    dualDelivered++;
                }
            }
            if (dualDelivered >= TOTAL_BLOCKS) dualDone = true;
        }

        draw();

        if (baselineDone && dualDone) {
            phase = 'done';
            btn.textContent = 'Restart';
            btn.disabled = false;
            // One more draw to show final state
            draw();
            return;
        }

        animId = requestAnimationFrame(tick);
    }

    function start() {
        if (animId) cancelAnimationFrame(animId);
        reset();
        phase = 'running';
        btn.textContent = 'Running...';
        btn.disabled = true;
        draw();
        animId = requestAnimationFrame(tick);
    }

    // Initial draw
    draw();

    btn.addEventListener('click', function() {
        start();
    });
})();
</script>

<h3>PE Read Path (Conventional)</h3>

<p>This is the standard path. KV-Cache is read from persistent storage into the PE's DRAM buffer via its storage NIC. From DRAM, it moves through the CNIC to GPU HBM. During layerwise prefill, this happens one attention layer at a time: the PE reads one layer's KV-Cache into HBM, computes that attention layer, then loads the next.</p>

<h3>DE Read Path (New)</h3>

<p>This is DualPath's contribution. KV-Cache is read from storage into a decode engine's DRAM buffer (via the DE's otherwise-idle storage NIC). Then the DE's CNIC sends it to the PE's CNIC via high-bandwidth RDMA over the compute network. From there it flows into PE GPU HBM the same as the PE path.</p>

<div class="note">
    <strong>Intuition:</strong> Instead of having one door into the building for deliveries (PE storage NIC), you open a second door through the neighboring building (DE storage NIC) and use an internal hallway (compute network RDMA) to move packages between buildings. The hallway has massive capacity and is only used in short bursts, so adding delivery traffic barely affects it.
</div>

<h3>Layerwise Prefill</h3>

<p>A critical enabler for DualPath is <strong>layerwise prefill</strong>: instead of loading the entire KV-Cache (all layers) into GPU HBM at once, the system loads and processes one layer at a time. This is necessary because HBM capacity is limited, but it also means KV-Cache data is transferred in many small chunks (one layer's worth at a time).</p>

<p>This creates a design challenge. A model with 30 layers means 30 sequential load-compute cycles per request. Each load is a small transfer that must be efficiently overlapped with computation. DualPath uses <strong>Layer Blocks</strong> (shape: <code>[1, tokens, bytes]</code>) for these per-layer transfers and <strong>Full Blocks</strong> (shape: <code>[layer, tokens, bytes]</code>) for storage interactions.</p>

<!-- ============================================================ -->
<h2 id="traffic">5. CNIC-Centric Traffic Manager</h2>

<p>Adding a second data path creates a practical problem: KV-Cache transfer traffic now shares the compute network and PCIe bus with latency-sensitive model execution operations (AllToAll for expert parallel, ReduceScatter for tensor parallel). These collective operations happen in sub-millisecond bursts and are critical for end-to-end latency.</p>

<h3>The Problem with Existing Approaches</h3>

<p>Existing GPU data transfer technologies (GPUDirect Storage, CUDA copy engine) don't provide fine-grained QoS control. They can't prevent KV-Cache traffic from interfering with collective communications. The paper measured CUDA copy engine overhead at 5-7 microseconds per operation, while RDMA write submission takes only ~1 microsecond.</p>

<h3>CNIC as Central Traffic Controller</h3>

<p>DualPath routes <em>all</em> GPU data traffic (including local H2D/D2H copies) through the GPU's paired CNIC using GPUDirect RDMA. This seems like a detour, but it has a key benefit: the CNIC becomes the single point of QoS control for all PCIe traffic.</p>

<p>For InfiniBand, DualPath uses <strong>Virtual Lanes (VLs)</strong> to isolate traffic:</p>

<ul>
    <li>Model inference traffic (AllToAll, etc.) gets a dedicated high-priority VL with ~99% of bandwidth via Weighted Round Robin scheduling.</li>
    <li>KV-Cache transfer traffic gets a low-priority VL that opportunistically uses idle bandwidth.</li>
</ul>

<p>This ensures KV-Cache traffic is essentially invisible to model execution. The same principle works on RoCE networks using Traffic Classes and DSCP markings.</p>

<!-- ============================================================ -->
<h2 id="scheduler">6. Adaptive Request Scheduler</h2>

<p>With two paths available, the system needs to decide for each request: which PE handles prefill, which DE handles decode, and which path loads the KV-Cache. A naive policy (e.g., round-robin assignment or always routing through the PE path first) can overload one side's storage NIC while the other sits idle, recreating the original bottleneck.</p>

<h3>Inter-Engine Scheduling</h3>

<p>The scheduler assigns each incoming request to a PE-DE pair. It picks PEs by checking two things: how backed up is the GPU (how many tokens are queued for computation), and how backed up is the disk (how many tokens are waiting to be read from storage).</p>

<ul>
    <li>If a PE's GPU queue is too long (more than ~5 seconds of work), it's <strong>overloaded</strong> and skipped entirely.</li>
    <li>If both the GPU queue and the disk queue are short (disk queue under ~3 seconds of reads), it's a <strong>best candidate</strong> and preferred.</li>
    <li>If the GPU has room but the disk queue is long, it's a <strong>fallback candidate</strong>, used only when no best candidates are available.</li>
</ul>

<p>The intuition: avoid sending new work to a PE that's already drowning in either compute or I/O. Prefer PEs that are light on both.</p>

<p>For DEs, scheduling is two-phase: first spread requests evenly across DE groups (by total token count), then within a group pick the DE with the most free HBM.</p>

<h3>KV-Cache Read Task Scheduling</h3>

<p>After a PE-DE pair is selected, the scheduler checks which side has the shorter storage read queue and routes the KV-Cache read through that side. This simple heuristic naturally balances storage NIC utilization across the cluster.</p>

<h3>Intra-Engine Scheduling</h3>

<p>Within a PE, the system uses a <strong>compute quota</strong> to decide how many requests to include in each forward batch. Each request is described by a pair $(cached, bsz)$: how many tokens have KV-Cache already available, and how many tokens need fresh computation. The scheduler estimates attention layer execution time and packs requests until reaching the quota.</p>

<p>If a request would exceed the quota, binary search finds a smaller $bsz'$ and performs <strong>chunked prefill</strong> on the remainder. This keeps GPU utilization high while preventing individual large requests from creating stragglers in data-parallel setups.</p>

<!-- ============================================================ -->
<h2 id="results">7. Results</h2>

<h3>Offline Inference (RL Training Rollouts)</h3>

<table>
    <thead>
        <tr>
            <th>Model</th>
            <th>Config</th>
            <th>DualPath Speedup</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>DS 660B</td>
            <td>2P4D, 32K-64K context</td>
            <td>Up to 1.87x over baseline</td>
        </tr>
        <tr>
            <td>DS 27B</td>
            <td>1P1D</td>
            <td>Up to 1.78x over baseline</td>
        </tr>
        <tr>
            <td>Qwen 32B</td>
            <td>1P2D</td>
            <td>Similar trends to DS 27B</td>
        </tr>
    </tbody>
</table>

<p>On DS 660B, DualPath nearly matches the Oracle configuration. Oracle is an idealized baseline where all KV-Cache is assumed to already be in GPU HBM (zero storage I/O). Matching Oracle means DualPath hides storage latency so effectively that the system behaves as if KV-Cache loading were free.</p>

<h3>Online Serving</h3>

<p>DualPath achieves <strong>1.96x higher agent runs per second</strong> on average compared to the baseline. Key latency metrics:</p>

<ul>
    <li><strong>TTST</strong> is comparable to baseline, meaning no additional decode overhead.</li>
    <li><strong>TTFT</strong> remains stable as load increases, while baseline's TTFT spikes due to storage NIC saturation.</li>
</ul>

<h3>Ablation: What Contributes What</h3>

<p>On DS 660B with 64K context and 2048 agents, each component contributes cumulatively to JCT reduction: layerwise prefill alone gives 17% by hiding HBM transfer overhead, dual-path loading adds another 21% (total 38%) by doubling available storage bandwidth, and the adaptive scheduler adds another 8% (total 46%) by balancing load effectively.</p>

<h3>Large-Scale Scalability</h3>

<p>Tested up to 1,152 GPUs. Scaling from 2P4D (2K agents) to 48P96D (48K agents) achieves near-linear speedup with comparable JCT. For online serving, a 44P88D configuration achieves 22x throughput (8.8 vs 0.4 APS) while maintaining similar latency. Scheduler CPU stays below 10 cores.</p>

<!-- ============================================================ -->
<h2 id="practical">8. Practical Notes</h2>

<p><strong>When does DualPath help most?</strong> When append lengths are short and context is long (high cache-hit ratio). With longer appends, GPU compute becomes the bottleneck instead of storage I/O, and DualPath's advantage shrinks. The paper shows that at 3x append length scaling, Basic performance approaches Oracle.</p>

<p><strong>P/D ratio matters.</strong> DualPath and Basic perform comparably when they have equivalent total storage bandwidth. A Basic 1P2D system (one prefill node, two decode nodes) has the same storage bandwidth as DualPath 2P1D. The advantage of DualPath is that it can exploit any P/D ratio without wasting storage bandwidth on the idle side.</p>

<div class="warning">
    <strong>Key limitation:</strong> DualPath adds DRAM pressure on decode engines (DE buffer) and introduces additional PCIe traffic. The CNIC-centric approach, while enabling QoS, adds a small detour compared to direct GPUDirect Storage or CUDA copy. For small models where PCIe bandwidth is already tight, this overhead may not be negligible.
</div>

<p><strong>Implementation cost is modest.</strong> The entire DualPath implementation is approximately 5,000 lines of code on top of their existing inference framework, using FlashMLA, DeepGEMM, and DeepEP.</p>

<p><strong>Storage backend.</strong> All experiments use 3FS (DeepSeek's distributed filesystem). The 3FS storage NIC has no internal DRAM cache and can saturate its 400 Gbps bandwidth. DualPath could be combined with a distributed DRAM cache (like Mooncake), but the paper notes the marginal performance gain is small.</p>

<p><strong>Bottleneck-free range.</strong> The paper proves analytically that for typical configurations ($g=8$ GPUs per node, $s=1$ storage NIC, $M \approx 500$ GB/s memory bandwidth, $Bs \approx 50$ GB/s storage bandwidth), DualPath is bottleneck-free when $\frac{1}{7} \leq P/D \leq \frac{7}{2}$. This covers most practical deployments.</p>

<hr>

<p style="color: var(--muted); font-size: 0.9em;">
    <strong>Reference:</strong>
    Shang et al., <a href="https://arxiv.org/pdf/2602.21548">DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference</a> (2025)
</p>

<div class="share-bottom">
    <p>Found this useful? Help someone else find it too.</p>
    <div class="share-buttons">
        <a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fmesuvash.github.io%2Fblog%2F2026%2Fdualpath%2F&text=DualPath%3A%20Breaking%20the%20Storage%20Bandwidth%20Bottleneck%20in%20Agentic%20LLM%20Inference" target="_blank" rel="noopener" title="Share on X">
            <svg viewBox="0 0 24 24"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
        </a>
        <a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fmesuvash.github.io%2Fblog%2F2026%2Fdualpath%2F" target="_blank" rel="noopener" title="Share on LinkedIn">
            <svg viewBox="0 0 24 24"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
        </a>
        <a href="https://www.reddit.com/submit?url=https%3A%2F%2Fmesuvash.github.io%2Fblog%2F2026%2Fdualpath%2F&title=DualPath%3A%20Breaking%20the%20Storage%20Bandwidth%20Bottleneck%20in%20Agentic%20LLM%20Inference" target="_blank" rel="noopener" title="Share on Reddit">
            <svg viewBox="0 0 24 24"><path d="M12 0A12 12 0 000 12a12 12 0 0012 12 12 12 0 0012-12A12 12 0 0012 0zm5.01 4.744c.688 0 1.25.561 1.25 1.249a1.25 1.25 0 01-2.498.056l-2.597-.547-.8 3.747c1.824.07 3.48.632 4.674 1.488.308-.309.73-.491 1.207-.491.968 0 1.754.786 1.754 1.754 0 .716-.435 1.333-1.01 1.614a3.111 3.111 0 01.042.52c0 2.694-3.13 4.87-7.004 4.87-3.874 0-7.004-2.176-7.004-4.87 0-.183.015-.366.043-.534A1.748 1.748 0 014.028 12c0-.968.786-1.754 1.754-1.754.463 0 .898.196 1.207.49 1.207-.883 2.878-1.43 4.744-1.487l.885-4.182a.342.342 0 01.14-.197.35.35 0 01.238-.042l2.906.617a1.214 1.214 0 011.108-.701zM9.25 12C8.561 12 8 12.562 8 13.25c0 .687.561 1.248 1.25 1.248.687 0 1.248-.561 1.248-1.249 0-.688-.561-1.249-1.249-1.249zm5.5 0c-.687 0-1.248.561-1.248 1.25 0 .687.561 1.248 1.249 1.248.688 0 1.249-.561 1.249-1.249 0-.687-.562-1.249-1.25-1.249zm-5.466 3.99a.327.327 0 00-.231.094.33.33 0 000 .463c.842.842 2.484.913 2.961.913.477 0 2.105-.056 2.961-.913a.361.361 0 000-.463.327.327 0 00-.231-.094c-.06 0-.12.02-.169.07-.572.573-1.7.76-2.546.76-.846 0-1.989-.187-2.561-.76a.291.291 0 00-.184-.07z"/></svg>
        </a>
    </div>
    <p class="follow-links">Follow me on <a href="https://x.com/suvsh" target="_blank" rel="noopener">X</a> and <a href="https://www.linkedin.com/in/suvashsedhain/" target="_blank" rel="noopener">LinkedIn</a></p>
</div>

</article>

  <div id="disqus_thread"></div>
  <script>
    var disqus_shortname = 'ssedhain';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

</div>

<footer class="site-footer">
  <div class="container">
    &copy; 2026 Suvash Sedhain
  </div>
</footer>

</body>
</html>